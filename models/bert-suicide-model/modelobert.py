# -*- coding: utf-8 -*-
"""ModeloBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EVQoXmH7sb0vY8o7eDMhtOi2Arnq4xmK
"""

!pip install -q  datasets accelerate scikit-learn evaluate
!pip install -U transformers

import torch

print("Versión de PyTorch:", torch.__version__)
print("GPU disponible:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

from google.colab import files
import os

print("Subir tu archivo kaggle.json")
uploaded = files.upload()

os.makedirs("/root/.kaggle", exist_ok=True)

for fname in uploaded.keys():
    os.rename(fname, "/root/.kaggle/kaggle.json")

!chmod 600 /root/.kaggle/kaggle.json

os.makedirs("/content/datasets", exist_ok=True)

!kaggle datasets download -d akshita0560/suicide-detection -p /content/datasets

!unzip -o /content/datasets/suicide-detection.zip -d /content/datasets

print("\nArchivos encontrados en /content/datasets:")
!ls -R /content/datasets

import pandas as pd
from sklearn.model_selection import train_test_split

csv_path = "/content/datasets/file1.csv"
df = pd.read_csv(csv_path)

print("Primeras filas:")
display(df.head())

print("\nColumnas disponibles:")
print(df.columns.tolist())

TEXT_COL = "text"
LABEL_COL = "class"

if TEXT_COL not in df.columns or LABEL_COL not in df.columns:
    raise ValueError("Los nombres no están en df.columns")

df = df.dropna(subset=[TEXT_COL, LABEL_COL])

if df[LABEL_COL].dtype == "object":
    print("\nValue counts originales de la etiqueta:")
    print(df[LABEL_COL].value_counts())

    mapping = {
        "non-suicide": 0,
        "suicide": 1,
        "negative": 0,
        "positive": 1
    }
    df[LABEL_COL] = df[LABEL_COL].map(mapping)

df[LABEL_COL] = df[LABEL_COL].astype(int)

X = df[TEXT_COL].astype(str)
y = df[LABEL_COL].astype(int)

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

len(X_train), len(X_test)

from datasets import Dataset, DatasetDict

train_df = pd.DataFrame({ "text": X_train, "label": y_train })
test_df  = pd.DataFrame({ "text": X_test,  "label": y_test  })

train_ds = Dataset.from_pandas(train_df, preserve_index=False)
test_ds  = Dataset.from_pandas(test_df,  preserve_index=False)

dataset = DatasetDict({
    "train": train_ds,
    "test":  test_ds
})

dataset

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "bert-base-multilingual-cased"
num_labels = 2

tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

tokenized_dataset = dataset.map(tokenize_batch, batched=True)

tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset.set_format("torch")

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels
)

from transformers import TrainingArguments, Trainer
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    f1  = f1_score(labels, preds, average="weighted")
    return {"accuracy": acc, "f1_weighted": f1}

batch_size = 16

training_args = TrainingArguments(
    output_dir="/content/bert-suicide-checkpoints",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    learning_rate=2e-5,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_steps=50,
    do_train=True,
    do_eval=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import os

os.environ["WANDB_MODE"] = "disabled"
os.environ["WANDB_DISABLED"] = "true"

print("W&B está desactivado.")

trainer.train()
metrics = trainer.evaluate()
print(metrics)

import numpy as np
from sklearn.metrics import classification_report

predictions = trainer.predict(tokenized_dataset["test"])
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=-1)

print(classification_report(y_true, y_pred, digits=3))

import torch

def predict_texts(texts):
    device = next(model.parameters()).device

    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=128,
        return_tensors="pt"
    ).to(device)

    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()

    for txt, p in zip(texts, probs):
        print("Texto:", txt)
        print("Prob no suicida:", float(p[0]))
        print("Prob suicida:", float(p[1]))
        print("-" * 60)

ejemplos = [
    "Hoy fue un buen día, me siento tranquilo.",
    "No veo sentido en seguir viviendo, quiero terminar con todo."
]

predict_texts(ejemplos)

output_dir = "/content/bert-suicide-model"

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

print("Modelo y tokenizer guardados en:", output_dir)

import shutil
from google.colab import files

shutil.make_archive("/content/bert-suicide-model", "zip", "/content/bert-suicide-model")
files.download("/content/bert-suicide-model.zip")